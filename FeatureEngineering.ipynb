{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Feature Engineering\n",
    " This section will discuss the feature engineering method.\n",
    "\n",
    " In this study, we extract the features from four different views, thus, we have totally 85 features within four categories.\n",
    "\n",
    " The categories are \"Expression\", \"Epigenetic\", \"Genomic\" and \"Network\".\n",
    "\n",
    " After observation, we have found that these features have different number orders and some of them may have the variances with very low value, which may be though of repeated ones.\n",
    " By this minner, we should do some feature engineering.\n",
    "\n",
    " We employed the Laplacian Score function to score each features in its related category, and then use these scores to filter the features (lower score means the feature is more important):\n",
    " + The feature with low score will be retained.\n",
    " + The feature with high score should be avoided.\n",
    "\n",
    " The Laplacian Score is implemented by the Python Scikit-feature package mentioned [here](https://github.com/jundongl/scikit-feature)\n",
    "\n",
    " ### Code\n",
    "\n",
    " First, we should load the data with the category:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def LoadData(DataPath, PositivePath, NegativePath, Seed):\n",
    "    \"\"\"\n",
    "    A function to load the data from the given datafile\n",
    "    Input: TotalDataPath, PositiveNamePath, NegativeNamePath, and random seed for shuffle.\n",
    "    Output: TrainingData(DataFrame), TrainingLabel(List) and ValidSet(DataFrame). Please note that the index in DataFrame are not reset.\n",
    "\n",
    "    \"\"\"\n",
    "    Data = pd.read_csv(DataPath)\n",
    "    PositiveData = pd.read_csv(PositivePath, header=None).iloc[:,0].tolist()\n",
    "    NegativeData = pd.read_csv(NegativePath, header=None).iloc[:,0].tolist()\n",
    "    # Generate the Positive Index\n",
    "    PositiveIndex = []\n",
    "    for i in range(len(PositiveData)):\n",
    "        index = Data[Data['Gene_ID'] == PositiveData[i]].index.tolist()\n",
    "        PositiveIndex.extend(index)\n",
    "    # Generate the negative index\n",
    "    NegativeIndex = []\n",
    "    for i in range(len(NegativeData)):\n",
    "        index = Data[Data['Gene_ID'] == NegativeData[i]].index.tolist()\n",
    "        NegativeIndex.extend(index)\n",
    "\n",
    "    # Generate the data that has the label\n",
    "    UsefulIndex = PositiveIndex+NegativeIndex\n",
    "    TrainingData = Data.iloc[UsefulIndex, :]\n",
    "    ValidData = Data.drop(UsefulIndex)\n",
    "    ValidName = ValidData['Gene_ID']\n",
    "    ValidData = ValidData.drop(['Gene_ID'], axis = 1)\n",
    "\n",
    "    # Generate the training data label\n",
    "    P_Label = [1 for i in range(len(PositiveIndex))]\n",
    "    N_Label = [0 for i in range(len(NegativeIndex))]\n",
    "    TrainingLabel = P_Label + N_Label\n",
    "\n",
    "    TrainingData = TrainingData.assign(Label = TrainingLabel)\n",
    "    TrainingData = TrainingData.sample(frac=1, random_state=Seed)\n",
    "\n",
    "    TrainingLabel = TrainingData['Label'].tolist()\n",
    "    TrainingName = TrainingData['Gene_ID']\n",
    "    TrainingData = TrainingData.drop(['Label', 'Gene_ID'], axis=1)\n",
    "\n",
    "\n",
    "    return TrainingData, TrainingLabel, ValidData\n",
    "\n",
    "def GetTypeData(varpath):\n",
    "    DataFilePath = 'OriginalData/Data.csv'\n",
    "    PositivePath = 'OriginalData/Positive.csv'\n",
    "    NegativePath = 'OriginalData/Negative.csv'\n",
    "\n",
    "    ShuffleSeed = 442\n",
    "    X, y, Valid = LoadData(DataFilePath, PositivePath, NegativePath, ShuffleSeed)\n",
    "    X = X.fillna(0)\n",
    "    Valid = Valid.fillna(0)\n",
    "\n",
    "    _TypeFeatureName = pd.read_csv(varpath).columns.values\n",
    "    Type_X = X[_TypeFeatureName]\n",
    "    Type_Valid = Valid[_TypeFeatureName]\n",
    "    \n",
    "    return Type_X, Type_Valid\n",
    "\n",
    "TypeOnePath = \"FeatureVariance/Feature_Epigenetic.csv\"\n",
    "TypeOneTrain, TypeOneValid = GetTypeData(TypeOnePath)\n",
    "\n",
    "TypeTwoPath = \"FeatureVariance/Feature_Expression.csv\"\n",
    "TypeTwoTrain, TypeTwoValid = GetTypeData(TypeTwoPath)\n",
    "\n",
    "TypeThrPath = \"FeatureVariance/Feature_Genomic.csv\"\n",
    "TypeThrTrain, TypeThrValid = GetTypeData(TypeThrPath)\n",
    "\n",
    "TypeFourPath = \"FeatureVariance/Feature_Network.csv\"\n",
    "TypeFourTrain, TypeFourValid = GetTypeData(TypeFourPath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Then, The Laplacian Score should be calculated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skfeature.function.similarity_based import lap_score\n",
    "from skfeature.utility import construct_W\n",
    "def LapScoreCal(Data):\n",
    "    Data = np.asarray(Data)\n",
    "    kwargs_W = {\"metric\": \"euclidean\", \"neighbor_mode\": \"knn\", \"weight_mode\": \"heat_kernel\", \"k\": 5, 't': 1}\n",
    "    W = construct_W.construct_W(Data, **kwargs_W)\n",
    "    LapScore = lap_score.lap_score(Data, W=W)\n",
    "    RankScore = lap_score.feature_ranking(LapScore)\n",
    "\n",
    "    return LapScore, RankScore\n",
    "\n",
    "LapScore_One, RankOne = LapScoreCal(TypeOneTrain)\n",
    "LapScore_Two, RankTwo = LapScoreCal(TypeTwoTrain)\n",
    "LapScore_Thr, RankThr = LapScoreCal(TypeThrTrain)\n",
    "LapScore_Four, RankFour = LapScoreCal(TypeFourTrain)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Now we get the scores and corresponding orders.\n",
    " We can plot them to see the score distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "ScoreList = [LapScore_One, LapScore_Two, LapScore_Thr, LapScore_Four]\n",
    "RankList = [RankOne, RankTwo, RankThr, RankFour]\n",
    "TypeList = ['Epigenetic', 'Expression', 'Genomic', 'Network']\n",
    "plt.figure(figsize=(6,5))\n",
    "for i in range(4):\n",
    "    plt.subplot(2,2, i+1)\n",
    "    plt.plot(np.arange(0, len(ScoreList[i])), ScoreList[i], \"o\")\n",
    "    plt.title(\"Laplacian Score\\n%s\"%(TypeList[i]))\n",
    "plt.subplots_adjust(hspace=0.13)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " It is clear that each category of features can be divided into three parts by the first two largest difference, which is as follows,\n",
    " ![LapScore](2.png)\n",
    "\n",
    " Just remove the features with high score is not desirable because these features also contain some information.\n",
    " In this way, we intergate the high-score features by mean in each of the high-score space to obtain two more intergation features in each category.\n",
    "\n",
    " Finally, we have got 53 features, 22 for \"Epigenetic\", 14 for \"Expression\", 11 for \"Genomic\" and 6 for \"Network\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
